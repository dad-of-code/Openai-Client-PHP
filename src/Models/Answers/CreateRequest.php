<?php

/**
 * Copyright (c) 2022 Tectalic (https://tectalic.com)
 *
 * For copyright and license information, please view the LICENSE file that was distributed with this source code.
 *
 * Please see the README.md file for usage instructions.
 */

declare(strict_types=1);

namespace Tectalic\OpenAi\Models\Answers;

use Tectalic\OpenAi\Models\AbstractModel;

final class CreateRequest extends AbstractModel
{
    /**
     * List of required property names.
     *
     * These properties must all be set when this Model is instantiated.
     */
    protected const REQUIRED = ['model', 'question', 'examples', 'examples_context'];

    /** @var bool */
    protected $ignoreMissing = false;

    /**
     * ID of the model to use for completion. You can select one of ada, babbage,
     * curie, or davinci.
     *
     * @var string
     */
    public $model;

    /**
     * Question to get answered.
     *
     * Example: 'What is the capital of Japan?'
     *
     * @var string
     */
    public $question;

    /**
     * List of (question, answer) pairs that will help steer the model towards the tone
     * and answer format you'd like. We recommend adding 2 to 3 examples.
     *
     * @var array
     */
    public $examples;

    /**
     * A text snippet containing the contextual information used to generate the
     * answers for the examples you provide.
     *
     * Example: 'Ottawa, Canada\'s capital, is located in the east of southern Ontario, near the city of MontrÃ©al and the U.S. border.'
     *
     * @var string
     */
    public $examples_context;

    /**
     * List of documents from which the answer for the input question should be
     * derived. If this is an empty list, the question will be answered based on the
     * question-answer examples.
     * You should specify either documents or a file, but not both.
     *
     * @var string[]|null
     */
    public $documents;

    /**
     * The ID of an uploaded file that contains documents to search over. See upload
     * file for how to upload a file of the desired format and purpose.
     * You should specify either documents or a file, but not both.
     *
     * @var string|null
     */
    public $file;

    /**
     * ID of the model to use for Search. You can select one of ada, babbage, curie, or
     * davinci.
     *
     * Default Value: 'ada'
     *
     * @var string|null
     */
    public $search_model;

    /**
     * The maximum number of documents to be ranked by Search when using file. Setting
     * it to a higher value leads to improved accuracy but with increased latency and
     * cost.
     *
     * Default Value: 200
     *
     * @var int|null
     */
    public $max_rerank;

    /**
     * What sampling temperature to use. Higher values mean the model will take more
     * risks and value 0 (argmax sampling) works better for scenarios with a
     * well-defined answer.
     *
     * Default Value: 0
     *
     * @var float|int|null
     */
    public $temperature;

    /**
     * Include the log probabilities on the logprobs most likely tokens, as well the
     * chosen tokens. For example, if logprobs is 5, the API will return a list of the
     * 5 most likely tokens. The API will always return the logprob of the sampled
     * token, so there may be up to logprobs+1 elements in the response.
     * The maximum value for logprobs is 5. If you need more than this, please contact
     * us through our Help center and describe your use case.
     * When logprobs is set, completion will be automatically added into expand to get
     * the logprobs.
     *
     * Default Value: null
     *
     * @var int|null
     */
    public $logprobs;

    /**
     * The maximum number of tokens allowed for the generated answer
     *
     * Default Value: 16
     *
     * @var int|null
     */
    public $max_tokens;

    /**
     * Up to 4 sequences where the API will stop generating further tokens. The
     * returned text will not contain the stop sequence.
     *
     * Default Value: null
     *
     * @var mixed
     */
    public $stop;

    /**
     * How many answers to generate for each question.
     *
     * Default Value: 1
     *
     * @var int|null
     */
    public $n;

    /**
     * Modify the likelihood of specified tokens appearing in the completion.
     * Accepts a json object that maps tokens (specified by their token ID in the GPT
     * tokenizer) to an associated bias value from -100 to 100. You can use this
     * tokenizer tool (which works for both GPT-2 and GPT-3) to convert text to token
     * IDs. Mathematically, the bias is added to the logits generated by the model
     * prior to sampling. The exact effect will vary per model, but values between -1
     * and 1 should decrease or increase likelihood of selection; values like -100 or
     * 100 should result in a ban or exclusive selection of the relevant token.
     * As an example, you can pass {"50256": -100} to prevent the <|endoftext|> token
     * from being generated.
     *
     * Default Value: null
     *
     * @var \Tectalic\OpenAi\Models\Answers\CreateRequestLogitBias|null
     */
    public $logit_bias;

    /**
     * A special boolean flag for showing metadata. If set to true, each document entry
     * in the returned JSON will contain a "metadata" field.
     * This flag only takes effect when file is set.
     *
     * Default Value: false
     *
     * @var bool|null
     */
    public $return_metadata;

    /**
     * If set to true, the returned JSON will include a "prompt" field containing the
     * final prompt that was used to request a completion. This is mainly useful for
     * debugging purposes.
     *
     * Default Value: false
     *
     * @var bool|null
     */
    public $return_prompt;

    /**
     * If an object name is in the list, we provide the full information of the object;
     * otherwise, we only provide the object ID. Currently we support completion and
     * file objects for expansion.
     *
     * Default Value: []
     *
     * @var mixed
     */
    public $expand;

    /**
     * A unique identifier representing your end-user, which will help OpenAI to
     * monitor and detect abuse.
     *
     * Example: 'user-1234'
     *
     * @var string
     */
    public $user;
}
